{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nimport numpy as np\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Carregamento dos dados\nfor dirname, _, filenames in os.walk('/kaggle/input/nlp-getting-started'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nsubmission_df = pd.DataFrame(submission)\ntrain_df = pd.DataFrame(train)\ntest_df = pd.DataFrame(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:34.522787Z","iopub.execute_input":"2025-05-10T04:05:34.523445Z","iopub.status.idle":"2025-05-10T04:05:34.558750Z","shell.execute_reply.started":"2025-05-10T04:05:34.523423Z","shell.execute_reply":"2025-05-10T04:05:34.558194Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}],"execution_count":114},{"cell_type":"markdown","source":"**LIMPEZA DO TEXTO**","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nnltk.download('wordnet')\nnltk.download('punkt')  # Necessário para word_tokenize\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english') + ['u', 'im', 'c'])\n\ndef clean_text(mensage):\n    mensage = str(mensage).lower()\n    mensage = re.sub('\\[.*?\\]', '', mensage)\n    mensage = re.sub('https?://\\S+|www\\.\\S+', '', mensage)\n    mensage = re.sub(r'@\\w+', '', mensage)\n    mensage = re.sub('<.*?>+', '', mensage)\n    mensage = re.sub('[%s]' % re.escape(string.punctuation), '', mensage)\n    mensage = re.sub('\\n', '', mensage)\n    mensage = re.sub('\\w*\\d\\w*', '', mensage)\n    \n    tokens = word_tokenize(mensage)\n    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]    \n    \n    return ' '.join(clean_tokens)\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(clean_text)\n\n# 2. Remoção de stopwords\ndef remove_stopwords(mensage):\n    mensage = ' '.join(word for word in mensage.split(' ') if word not in stop_words)\n    return mensage\n\ntrain_df['text'] = train_df['text'].apply(remove_stopwords)\ntest_df['text'] = test_df['text'].apply(remove_stopwords)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:34.559872Z","iopub.execute_input":"2025-05-10T04:05:34.560097Z","iopub.status.idle":"2025-05-10T04:05:35.912556Z","shell.execute_reply.started":"2025-05-10T04:05:34.560082Z","shell.execute_reply":"2025-05-10T04:05:35.912029Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":115},{"cell_type":"markdown","source":"**DIVISÃO DOS DADOS**","metadata":{}},{"cell_type":"code","source":"x = train_df['text']\ny = train_df['target']\n\n# Removendo o documento vazio (índice 217)\nempty_index = 217\nif empty_index in x.index:\n    x = x.drop(index=empty_index)\n    y = y.drop(index=empty_index)\n    y = y.reset_index(drop=True)\n    x = x.reset_index(drop=True)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.23, random_state=19)\n\n# Precisa resetar os índices do x_train e x_test\nx_train = x_train.reset_index(drop=True)\nx_test = x_test.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:35.913195Z","iopub.execute_input":"2025-05-10T04:05:35.913396Z","iopub.status.idle":"2025-05-10T04:05:35.923032Z","shell.execute_reply.started":"2025-05-10T04:05:35.913382Z","shell.execute_reply":"2025-05-10T04:05:35.922337Z"}},"outputs":[],"execution_count":116},{"cell_type":"markdown","source":"**MODELO SENTENCE TRANSFORMER**","metadata":{}},{"cell_type":"code","source":"model_name = 'all-mpnet-base-v2'\nembedding_model = SentenceTransformer(model_name)\n\n# Gerar os embeddings\nx_train_embeddings = embedding_model.encode(x_train, convert_to_tensor=True)\nx_test_embeddings = embedding_model.encode(x_test, convert_to_tensor=True)\n\n# Converter para tensores PyTorch (já estão como tensores após o encode)\nx_train_embeddings = x_train_embeddings.float()\nx_test_embeddings = x_test_embeddings.float()\ny_train_tensor = torch.tensor(y_train.values).float().unsqueeze(1)\ny_test_tensor = torch.tensor(y_test.values).float().unsqueeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:35.924674Z","iopub.execute_input":"2025-05-10T04:05:35.924862Z","iopub.status.idle":"2025-05-10T04:05:42.277651Z","shell.execute_reply.started":"2025-05-10T04:05:35.924847Z","shell.execute_reply":"2025-05-10T04:05:42.277130Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"142bdde3b6ff4ccd9e71a922edb2513f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d580b049a2544599a7db5f53a5698d0"}},"metadata":{}}],"execution_count":117},{"cell_type":"markdown","source":"**DATASET + DATALOADER**","metadata":{}},{"cell_type":"code","source":"class DisasterDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = embeddings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.embeddings[idx], self.labels[idx]\n\n    def check_distribution(self):\n            labels_np = self.labels.cpu().numpy()\n            unique, counts = np.unique(labels_np, return_counts=True)\n            return dict(zip(unique, counts))\n\ntrain_dataset = DisasterDataset(x_train_embeddings, y_train_tensor)\ntest_dataset = DisasterDataset(x_test_embeddings, y_test_tensor)\n\nprint(f\"Distribuição no dataset de treino: {train_dataset.check_distribution()}\")\n\n# Criar DataLoader\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.278329Z","iopub.execute_input":"2025-05-10T04:05:42.278574Z","iopub.status.idle":"2025-05-10T04:05:42.285584Z","shell.execute_reply.started":"2025-05-10T04:05:42.278548Z","shell.execute_reply":"2025-05-10T04:05:42.284794Z"}},"outputs":[{"name":"stdout","text":"Distribuição no dataset de treino: {0.0: 3373, 1.0: 2488}\n","output_type":"stream"}],"execution_count":118},{"cell_type":"markdown","source":"**DEFININDO O MODELO**","metadata":{}},{"cell_type":"code","source":"\nclass SimpleClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(SimpleClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n        self.relu = nn.LeakyReLU()\n        self.dropout = nn.Dropout(0.4)\n        \n        self.fc2 = nn.Linear(128, 64)  # ADICIONADO: Camada intermediária\n        self.batch_norm2 = nn.BatchNorm1d(64)\n        self.dropout2 = nn.Dropout(0.2)\n\n        self.fc3 = nn.Linear(64, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n        self._initialize_weights()\n\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.batch_norm1(x)\n        x = self.relu(x)\n        x = self.dropout1(x)\n        \n        x = self.fc2(x)\n        x = self.batch_norm2(x)\n        x = self.relu(x)\n        x = self.dropout2(x)\n        \n        x = self.fc3(x)\n        x = self.sigmoid(x)\n        return x\n\ninput_dim = x_train_embeddings.shape[1]\nmodel = SimpleClassifier(input_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.286411Z","iopub.execute_input":"2025-05-10T04:05:42.286643Z","iopub.status.idle":"2025-05-10T04:05:42.307427Z","shell.execute_reply.started":"2025-05-10T04:05:42.286629Z","shell.execute_reply":"2025-05-10T04:05:42.306843Z"}},"outputs":[],"execution_count":119},{"cell_type":"markdown","source":"**DEBBUGAR**","metadata":{}},{"cell_type":"code","source":"def train_with_debug(model, train_loader, criterion, optimizer, device, epochs=40):\n    losses = []\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        all_train_preds = []\n        all_train_labels = []\n        \n        for embeddings, labels in train_loader:\n            embeddings, labels = embeddings.to(device), labels.to(device)\n            \n            if epoch == 0 and len(all_train_preds) == 0:\n                print(f\"Amostra de embeddings: {embeddings[0][:5]}\")\n                print(f\"Amostra de labels: {labels[:5]}\")\n            \n            optimizer.zero_grad()\n            outputs = model(embeddings)\n            \n            if epoch == 0 and len(all_train_preds) == 0:\n                print(f\"Amostra de outputs: {outputs[:5]}\")\n            \n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            \n            preds = (outputs > 0.5).float()  # ALTERADO: Threshold para 0.5\n            all_train_preds.extend(preds.cpu().numpy())\n            all_train_labels.extend(labels.cpu().numpy())\n\n            if epoch % 5 == 0 or epoch == epochs - 1:\n                train_acc = accuracy_score(all_train_labels, all_train_preds)\n                train_f1 = f1_score(all_train_labels, all_train_preds)\n                avg_loss = epoch_loss / len(train_loader)\n                losses.append(avg_loss)\n                print(f'Época {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}')\n            \n            # ADICIONADO: Contar distribuição das previsões\n            unique, counts = np.unique(np.array(all_train_preds), return_counts=True)\n            pred_dist = dict(zip(unique, counts))\n            print(f\"Distribuição das previsões: {pred_dist}\")\n            \n            # Verificação de gradientes depois do backprop\n            if epoch % 10 == 0:\n                for name, param in model.named_parameters():\n                    if param.requires_grad:\n                        print(f\"{name} - grad_mean: {param.grad.abs().mean().item():.6f}\")\n    \n    # Plotar curva de loss\n    plt.figure(figsize=(10, 4))\n    plt.plot(range(0, epochs, 5), losses)\n    plt.xlabel('Época')\n    plt.ylabel('Loss')\n    plt.title('Curva de Loss durante Treinamento')\n    plt.grid(True)\n    plt.show()\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.308053Z","iopub.execute_input":"2025-05-10T04:05:42.308297Z","iopub.status.idle":"2025-05-10T04:05:42.329760Z","shell.execute_reply.started":"2025-05-10T04:05:42.308274Z","shell.execute_reply":"2025-05-10T04:05:42.329041Z"}},"outputs":[],"execution_count":120},{"cell_type":"markdown","source":"**FUNÇÃO DE PERDA E OTIMIZADOR**","metadata":{}},{"cell_type":"code","source":"def setup_training():\n    # Use GPU se disponível, caso contrário CPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Usando dispositivo: {device}\")\n    \n    # Definir hiperparâmetros e modelo\n    input_dim = x_train_embeddings.shape[1]\n    model = ImprovedClassifier(input_dim).to(device)\n    \n    # ALTERADO: Weight balancing para classes desbalanceadas\n    # Calcular weights para balancear as classes (apenas exemplo)\n    # pos_weight = torch.tensor([1.5])  # Ajuste este valor com base na distribuição real\n    # criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n    \n    # OU use BCELoss padrão se preferir\n    criterion = nn.BCELoss().to(device)\n    \n    # ALTERADO: Otimizador com melhores hiperparâmetros\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n    \n    # Treinar com debugging\n    model = train_with_debug(model, train_loader, criterion, optimizer, device, epochs=30)\n    \n    return model, device\n\n# Função de avaliação com análise detalhada\ndef evaluate_with_analysis(model, test_loader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_outputs = []  # Valor raw da sigmoide\n    \n    with torch.no_grad():\n        for embeddings, labels in test_loader:\n            embeddings, labels = embeddings.to(device), labels.to(device)\n            outputs = model(embeddings)\n            \n            # Salvar tanto os valores raw quanto as previsões binárias\n            all_outputs.extend(outputs.cpu().numpy())\n            preds = (outputs > 0.5).float()  # ALTERADO: Threshold para 0.5\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Converter para arrays numpy para análise\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_outputs = np.array(all_outputs)\n    \n    # ADICIONADO: Verifica se há variação nas previsões\n    unique_preds = np.unique(all_preds)\n    print(f\"Valores únicos nas previsões: {unique_preds}\")\n    \n    # ADICIONADO: Histograma dos scores (valores da sigmoide)\n    plt.figure(figsize=(10, 4))\n    plt.hist(all_outputs, bins=20)\n    plt.xlabel('Score de Previsão')\n    plt.ylabel('Contagem')\n    plt.title('Distribuição dos Scores de Previsão')\n    plt.show()\n    \n    # Calcular métricas\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    \n    print(f'Acurácia: {accuracy:.4f}')\n    print(f'F1-score: {f1:.4f}')\n    print('\\nRelatório de Classificação:')\n    print(classification_report(all_labels, all_preds))\n    \n    # Matriz de confusão\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='magma',\n                xticklabels=['Não Desastre', 'Desastre'], \n                yticklabels=['Não Desastre', 'Desastre'])\n    plt.xlabel('Previsto')\n    plt.ylabel('Real')\n    plt.title('Matriz de Confusão')\n    plt.show()\n    \n    return accuracy, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:07:24.288873Z","iopub.execute_input":"2025-05-10T04:07:24.289589Z","iopub.status.idle":"2025-05-10T04:07:24.298890Z","shell.execute_reply.started":"2025-05-10T04:07:24.289563Z","shell.execute_reply":"2025-05-10T04:07:24.298112Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"# Definir a função de perda e o otimizador\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Treinar o modelo\nepochs = 40\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device\nmodel.to(device) # Move model para device\ncriterion.to(device) \n\nfor epoch in range(epochs):\n    model.train()\n    for embeddings, labels in train_loader:\n        embeddings, labels = embeddings.to(device), labels.to(device) \n        optimizer.zero_grad()\n        outputs = model(embeddings)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:07:41.909523Z","iopub.execute_input":"2025-05-10T04:07:41.910235Z","iopub.status.idle":"2025-05-10T04:07:41.959897Z","shell.execute_reply.started":"2025-05-10T04:07:41.910195Z","shell.execute_reply":"2025-05-10T04:07:41.958819Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3774446973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1056580147.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'SimpleClassifier' object has no attribute 'dropout1'"],"ename":"AttributeError","evalue":"'SimpleClassifier' object has no attribute 'dropout1'","output_type":"error"}],"execution_count":125},{"cell_type":"markdown","source":"**TREINO + AVALIAÇÃO DO MODELO**","metadata":{}},{"cell_type":"code","source":"model.eval()\nall_preds = []\nwith torch.no_grad():\n    for embeddings, labels in test_loader:\n        embeddings, labels = embeddings.to(device), labels.to(device)  \n        outputs = model(embeddings)\n        preds = (outputs > 0.4462).float()\n        all_preds.extend(preds.cpu().numpy())\n\n\naccuracy_pytorch = accuracy_score(y_test, all_preds)\nf1_pytorch = f1_score(y_test, all_preds)\n\nprint(f'Acurácia com PyTorch: {accuracy_pytorch:.4f}')\nprint(f'F1-score com PyTorch: {f1_pytorch:.4f}')\n\nprint('\\nRelatório de Classificação (PyTorch):')\nprint(classification_report(y_test, all_preds))\n\ncm_pytorch = confusion_matrix(y_test, all_preds)\nplt.figure(figsize=(2,2))\nsns.heatmap(cm_pytorch, annot=True, fmt='d', cmap='magma',\n            xticklabels=['Não Desastre', 'Desastre'], yticklabels=['Não Desastre', 'Desastre'])\nplt.xlabel('Previsto (PyTorch)')\nplt.ylabel('Real')\nplt.title('Matriz de Confusão (PyTorch)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.425448Z","iopub.status.idle":"2025-05-10T04:05:42.425653Z","shell.execute_reply.started":"2025-05-10T04:05:42.425554Z","shell.execute_reply":"2025-05-10T04:05:42.425564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gerar embeddings para o conjunto de teste do Kaggle (usando o embedding_model treinado)\nx_test_kaggle = test_df['text']  # Usar o texto já limpo\nx_test_kaggle_embeddings = embedding_model.encode(x_test_kaggle, convert_to_tensor=True).float().to(device)\n\n# Fazer previsões no conjunto de teste do Kaggle\nmodel.eval()\nall_probabilities_kaggle = []\nall_predictions_kaggle = []\nwith torch.no_grad():\n    for embeddings in x_test_kaggle_embeddings:\n        output = model(embeddings.unsqueeze(0))  # Passar os embeddings pelo modelo\n        probability = torch.sigmoid(output).item()  # Obter probabilidade escalar\n        all_probabilities_kaggle.append(probability)\n        prediction = 1 if probability > 0.4462 else 0  # Classificar com o limiar\n        all_predictions_kaggle.append(prediction)\n\n# Criar o dataframe de submissão\nsubmission_df_kaggle = pd.DataFrame({'id': test_df['id'], 'target': all_predictions_kaggle})\nsubmission_df_kaggle.to_csv('submission.csv', index=False)\n\nprint(\"Arquivo de submissão 'submission.csv' criado com as previsões no conjunto de teste do Kaggle.\")\nprint(\"Primeiras 20 Previsões:\", all_predictions_kaggle[:20])\nprint(\"Primeiras 20 Probabilidades:\", all_probabilities_kaggle[:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.426816Z","iopub.status.idle":"2025-05-10T04:05:42.427171Z","shell.execute_reply.started":"2025-05-10T04:05:42.427008Z","shell.execute_reply":"2025-05-10T04:05:42.427022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**LOGISTIC REGRESSION**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# 7. Treinamento do modelo de Regressão Logística (aplicando o melhor parâmetro C encontrado anteriormente)\nlogistic_model = LogisticRegression(solver='liblinear', random_state=19, C=10)\nlogistic_model.fit(x_train_tfidf, y_train)\n\n# 8. Predições\ny_pred_logistic = logistic_model.predict(x_test_tfidf)\n\n# 9. Avaliação\naccuracy_logistic = accuracy_score(y_test, y_pred_logistic)\nprint(f'Acurácia do modelo de Regressão Logística: {accuracy_logistic:.4f}')\nprint('\\nRelatório de Classificação (Regressão Logística):')\nprint(classification_report(y_test, y_pred_logistic))\n\ncm_logistic = confusion_matrix(y_test, y_pred_logistic)\nplt.figure(figsize=(2,2))\nsns.heatmap(cm_logistic, annot=True, fmt='d', cmap='Blues',\n            xticklabels=logistic_model.classes_, yticklabels=logistic_model.classes_)\nplt.xlabel('Previsto (Regressão Logística)')\nplt.ylabel('Real')\nplt.title('Matriz de Confusão (Regressão Logística)')\nplt.show()\n\nf1_logistic = f1_score(y_test, y_pred_logistic)\nprint(f'F1-score da Regressão Logística: {f1_logistic:.4f}')\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.428104Z","iopub.status.idle":"2025-05-10T04:05:42.428307Z","shell.execute_reply.started":"2025-05-10T04:05:42.428208Z","shell.execute_reply":"2025-05-10T04:05:42.428216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# Define os valores de C que você quer testar\nparam_grid_logistic = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Cria o modelo de Regressão Logística\nlogistic_model = LogisticRegression(solver='liblinear', random_state=19)\n\n# Configura o GridSearchCV\ngrid_search_logistic = GridSearchCV(logistic_model, param_grid_logistic, cv=5, scoring='f1')\n\n# Executa a busca nos dados de treinamento\ngrid_search_logistic.fit(x_train_tfidf, y_train)\n\n# Melhores parâmetros encontrados\nprint(\"Melhores hiperparâmetros para Regressão Logística:\", grid_search_logistic.best_params_)\n\n# Melhor score (F1) obtido\nprint(\"Melhor F1-score para Regressão Logística:\", grid_search_logistic.best_score_)\n\n# Avalia o modelo com os melhores parâmetros no conjunto de teste\nbest_logistic_model = grid_search_logistic.best_estimator_\ny_pred_best_logistic = best_logistic_model.predict(x_test_tfidf)\n\nf1_best_logistic = f1_score(y_test, y_pred_best_logistic)\naccuracy_best_logistic = accuracy_score(y_test, y_pred_best_logistic)\n\nprint(f'Acurácia da Regressão Logística (melhores parâmetros): {accuracy_best_logistic:.4f}')\nprint(f'F1-score da Regressão Logística (melhores parâmetros): {f1_best_logistic:.4f}')\n\nprint(\"\\nRelatório de Classificação da Regressão Logística (melhores parâmetros):\")\nprint(classification_report(y_test, y_pred_best_logistic))\n\ncm_best_logistic = confusion_matrix(y_test, y_pred_best_logistic)\nplt.figure(figsize=(2,2))\nsns.heatmap(cm_best_logistic, annot=True, fmt='d', cmap='Blues',\n            xticklabels=best_logistic_model.classes_, yticklabels=best_logistic_model.classes_)\nplt.xlabel('Previsto')\nplt.ylabel('Real')\nplt.title('Matriz de Confusão (Regressão Logística - Melhores Parâmetros)')\nplt.show()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.429012Z","iopub.status.idle":"2025-05-10T04:05:42.429232Z","shell.execute_reply.started":"2025-05-10T04:05:42.429136Z","shell.execute_reply":"2025-05-10T04:05:42.429145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"XGBoost","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# 7. Treinamento do modelo XGBoost\nxgb_model = XGBClassifier(random_state=19)\nxgb_model.fit(x_train_tfidf, y_train)\n\n# 8. Predições\ny_pred_xgb = xgb_model.predict(x_test_tfidf)\n\n# 9. Avaliação\naccuracy_xgb = accuracy_score(y_test, y_pred_xgb)\nprint(f'Acurácia do modelo XGBoost: {accuracy_xgb:.4f}')\nprint('\\nRelatório de Classificação (XGBoost):')\nprint(classification_report(y_test, y_pred_xgb))\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\nplt.figure(figsize=(2,2))\nsns.heatmap(cm_xgb, annot=True, fmt='d', cmap='plasma',\n            xticklabels=xgb_model.classes_, yticklabels=xgb_model.classes_)\nplt.xlabel('Previsto (XGBoost)')\nplt.ylabel('Real')\nplt.title('Matriz de Confusão (XGBoost)')\nplt.show()\n\nf1_xgb = f1_score(y_test, y_pred_xgb)\nprint(f'F1-score do XGBoost: {f1_xgb:.4f}')\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.430120Z","iopub.status.idle":"2025-05-10T04:05:42.430405Z","shell.execute_reply.started":"2025-05-10T04:05:42.430234Z","shell.execute_reply":"2025-05-10T04:05:42.430254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SVM","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\nfrom sklearn.svm import SVC\n\n# Define os valores de C e os kernels que você quer testar\nparam_grid_svm = {'C': [0.1, 1, 10, 100],\n                  'kernel': ['linear', 'rbf']}\n\n# Cria o modelo SVM\nsvm_model = SVC(random_state=19)\n\n# Configura o GridSearchCV\ngrid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=5, scoring='f1')\n\n# Executa a busca nos dados de treinamento\ngrid_search_svm.fit(x_train_tfidf, y_train)\n\n# Melhores parâmetros encontrados\nprint(\"Melhores hiperparâmetros para SVM:\", grid_search_svm.best_params_)\n\n# Melhor score (F1) obtido\nprint(\"Melhor F1-score para SVM:\", grid_search_svm.best_score_)\n\n# Avalia o modelo com os melhores parâmetros no conjunto de teste\nbest_svm_model = grid_search_svm.best_estimator_\ny_pred_best_svm = best_svm_model.predict(x_test_tfidf)\n\nf1_best_svm = f1_score(y_test, y_pred_best_svm)\naccuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n\nprint(f'Acurácia do SVM (melhores parâmetros): {accuracy_best_svm:.4f}')\nprint(f'F1-score do SVM (melhores parâmetros): {f1_best_svm:.4f}')\n\nprint(\"\\nRelatório de Classificação do SVM (melhores parâmetros):\")\nprint(classification_report(y_test, y_pred_best_svm))\n\ncm_best_svm = confusion_matrix(y_test, y_pred_best_svm)\nplt.figure(figsize=(2,2))\nsns.heatmap(cm_best_svm, annot=True, fmt='d', cmap='viridis',\n            xticklabels=best_svm_model.classes_, yticklabels=best_svm_model.classes_)\nplt.xlabel('Previsto')\nplt.ylabel('Real')\nplt.title('Matriz de Confusão (SVM - Melhores Parâmetros)')\nplt.show()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.431299Z","iopub.status.idle":"2025-05-10T04:05:42.431602Z","shell.execute_reply.started":"2025-05-10T04:05:42.431446Z","shell.execute_reply":"2025-05-10T04:05:42.431459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# 2. Remoção de stopwords\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\nstop_words = set(stop_words)\n\n\ndef clean_text(mensage):\n    mensage = str(mensage).lower()\n    mensage = re.sub('\\[.*?\\]', '', mensage)\n    mensage = re.sub('https?://\\S+|www\\.\\S+', '', mensage)\n    mensage = re.sub('<.*?>+', '', mensage)\n    mensage = re.sub('[%s]' % re.escape(string.punctuation), '', mensage)\n    mensage = re.sub('\\n', '', mensage)\n    mensage = re.sub('\\w*\\d\\w*', '', mensage)\n    return mensage\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(clean_text)\n\n\n\ndef remove_stopwords(mensage):\n    mensage = ' '.join(word for word in mensage.split(' ') if word not in stop_words)\n    return mensage\n\ntrain_df['text'] = train_df['text'].apply(remove_stopwords)\ntest_df['text'] = test_df['text'].apply(remove_stopwords)\n\n# Divisão dos dados\nx = train_df['text']\ny = train_df['target']\n\n# Removendo o documento vazio (índice 217)\nempty_index = 217\nif empty_index in x.index:\n    x = x.drop(index=empty_index)\n    y = y.drop(index=empty_index)\n    # Precisa resetar o index\n    x = x.reset_index(drop=True)  \n    y = y.reset_index(drop=True) \n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.21, random_state=19)\n\n# Carregar o modelo Sentence Transformer\nmodel_name = 'all-mpnet-base-v2'\nembedding_model = SentenceTransformer(model_name)\n\n# Gerar os embeddings\n# Converter x_train e x_test para lista antes de  encode\nx_train_embeddings = embedding_model.encode(x_train.tolist(), convert_to_tensor=True)  \nx_test_embeddings = embedding_model.encode(x_test.tolist(), convert_to_tensor=True)  \n\n# Move the tensors to CPU and convert to NumPy arrays\nx_train_embeddings = x_train_embeddings.cpu().numpy()  \nx_test_embeddings = x_test_embeddings.cpu().numpy()\n\n# Treinar o modelo de Regressão Logística\nfrom sklearn.linear_model import LogisticRegression # Importing the missing module\nlogistic_model = LogisticRegression(random_state=19, solver='liblinear')\nlogistic_model.fit(x_train_embeddings, y_train)\n\n# Fazer as previsões\ny_pred_sentence_transformer = logistic_model.predict(x_test_embeddings)\n\n# Avaliar o modelo\naccuracy_sentence_transformer = accuracy_score(y_test, y_pred_sentence_transformer)\nf1_sentence_transformer = f1_score(y_test, y_pred_sentence_transformer)\n\nprint(f'Acurácia com Sentence Transformer: {accuracy_sentence_transformer:.4f}')\nprint(f'F1-score com Sentence Transformer: {f1_sentence_transformer:.4f}')\n\nprint('\\nRelatório de Classificação (Sentence Transformer):')\nprint(classification_report(y_test, y_pred_sentence_transformer))\n\ncm_sentence_transformer = confusion_matrix(y_test, y_pred_sentence_transformer)\nplt.figure(figsize=(2,2))\nsns.heatmap(cm_sentence_transformer, annot=True, fmt='d', cmap='viridis',\n            xticklabels=['Não Desastre', 'Desastre'], yticklabels=['Não Desastre', 'Desastre'])\nplt.xlabel('Previsto (Sentence Transformer)')\nplt.ylabel('Real')\nplt.title('Matriz de Confusão (Sentence Transformer)')\nplt.show()\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:05:42.432353Z","iopub.status.idle":"2025-05-10T04:05:42.432581Z","shell.execute_reply.started":"2025-05-10T04:05:42.432482Z","shell.execute_reply":"2025-05-10T04:05:42.432492Z"}},"outputs":[],"execution_count":null}]}