{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWF6HURHcQK1jlk09Sakoi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roger-Quinelato/Veggie/blob/main/PyTorch_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Motivation\n",
        "Pre-trained networks abound; why not leverage them? Many applications addressing real-world problems can be swiftly developed using a limited set of observations for model training. Without this tool, tens of thousands of images are required to train a network from scratch, rendering it impractical in the vast majority of cases.\n",
        "\n",
        "A convolutional neural network is intricate in its inner workings and details under the hood. However, this complexity should not deter us from employing libraries that abstract away such intricacies, thereby creating value for the companies and organizations we serve, or even showcasing our ability to utilize deep learning tools for solving potential real-world problems (and enriching our portfolios).\n",
        "\n",
        "Object of Study\n",
        "Let's Veggie requires a minimum viable product that can be testd in the company's branches. The problem to be addressed is the difficulty in classifying products in the stores, as the company is experiencing rapid growth and hiring many new salespeople! Many of them are unfamiliar with distinguishing between vegetables and fruits, and an application that can perform this classification could greatly assist in the company's expansion.\n",
        "\n",
        "We will begin with the classification of potatoes, carrots, tomatoes, and lemons.\n",
        "\n",
        "Creation and Segregation of Datasets\n",
        "With the images downloaded from Google Images, we can separate them into training, validation, and test sets.\n",
        "\n",
        "The project requires only one \"raw\" folder within the \"data\" directory, with a subfolder for each class to be trained (in our case, the vegetables we will use in the MVP)."
      ],
      "metadata": {
        "id": "nq4or8OnLTcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BAyIFD4qK2fa"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First we will separate the images into training, validation and test bases\n",
        "#It is interesting to leave a raw folder with the original data just in case we need it.\n",
        "\n",
        "directory_image_base = './data/raw'\n",
        "folder_with_vegetable_names = os.listdir('./data/raw')\n",
        "folder_with_vegetable_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "x7Dxzr0bLi8Y",
        "outputId": "eefb8979-dd80-4628-b8e9-2fea649f106b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/raw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-81741b2e1ab0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdirectory_image_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/raw'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfolder_with_vegetable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfolder_with_vegetable_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/raw'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a stratified separation\n",
        "quantity_per_label = {folder: len(os.listdir(os.path.join(directory_image_base, folder))) for folder in folder_with_vegetable_names}\n",
        "\n",
        "quantity_per_label"
      ],
      "metadata": {
        "id": "7D_c9LQGL9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the folders for training, validadation and test\n",
        "directory_processed_images = './data/processed/'\n",
        "\n",
        "dir_training = os.path.join(directory_processed_images, 'training')\n",
        "dir_validation = os.path.join(directory_processed_images, 'validation')\n",
        "dir_test = os.path.join(directory_processed_images, 'test')\n",
        "\n",
        "if not os.path.exists(dir_training):\n",
        "    os.makedirs(dir_training)\n",
        "\n",
        "if not os.path.exists(dir_validation):\n",
        "    os.makedirs(dir_validation)\n",
        "\n",
        "if not os.path.exists(dir_test):\n",
        "    os.makedirs(dir_test)"
      ],
      "metadata": {
        "id": "Eu0yRLZkMD9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Criando uma folder para cada class (batata, cenoura, limao, tomate)\n",
        "# dentro de training, validação e test\n",
        "\n",
        "for classe in folder_with_vegetable_names:\n",
        "    # os.path.join cria paths com os separadores corretos pra cada sistema operacional\n",
        "    # barra normal, barra invertida, isso muda do Windows pro Linux/Mac\n",
        "    dir_class_training = os.path.join(dir_training, classe)\n",
        "    dir_class_validation = os.path.join(dir_validation, classe)\n",
        "    dir_class_test = os.path.join(dir_test, classe)\n",
        "\n",
        "    # Efetivamente criando as folders de training, validação e test\n",
        "    # Testa primeiro se as folders já não existem\n",
        "    if not os.path.exists(dir_class_training):\n",
        "        os.makedirs(dir_class_training)\n",
        "\n",
        "    if not os.path.exists(dir_class_validation):\n",
        "        os.makedirs(dir_class_validation)\n",
        "\n",
        "    if not os.path.exists(dir_class_test):\n",
        "        os.makedirs(dir_class_test)\n",
        "\n",
        "    # caminho completo para a folder com imagens originais\n",
        "    folder_class = os.path.join(directory_image_base, classe)\n",
        "\n",
        "    # listando todos os archives de image para essa class\n",
        "    archives_class = os.listdir(folder_class)\n",
        "\n",
        "    # separando 80% para training e 20% para validação+test\n",
        "    training, valid_test = train_test_split(archives_class,\n",
        "                                           shuffle=True,\n",
        "                                           test_size=0.2,\n",
        "                                           random_state=42)\n",
        "\n",
        "    # separando os 20% da validação+test em 10% para validação e 10% para test\n",
        "    validation, test = train_test_split(valid_test, shuffle=True, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Não precisamos mais dessa lista temporária\n",
        "    del valid_test\n",
        "\n",
        "    print(f'{classe} - training: {len(training)} - valid: {len(validation)} - test: {len(test)} - total: {len(archives_class)}')\n",
        "\n",
        "    # Copiando os archives efetivamente para as folders de training, validação e test\n",
        "    for image_training in training:\n",
        "        orgin_path = os.path.join(directory_image_base, classe, image_training)\n",
        "        destination_path = os.path.join(dir_class_training, image_training)\n",
        "\n",
        "        shutil.copy(orgin_path, destination_path)\n",
        "\n",
        "    for image_validation in validation:\n",
        "        orgin_path = os.path.join(directory_image_base, classe, image_validation)\n",
        "        destination_path = os.path.join(dir_class_validation, image_validation)\n",
        "\n",
        "        shutil.copy(orgin_path, destination_path)\n",
        "\n",
        "    for image_test in test:\n",
        "        orgin_path  = os.path.join(directory_image_base, classe, image_test)\n",
        "        destination_path = os.path.join(dir_class_test, image_test)\n",
        "\n",
        "        shutil.copy(orgin_path, destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Wp8FZ3_9MIff",
        "outputId": "1c0ae18d-5499-437c-bec2-f1199533fc61"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'folder_with_vegetable_names' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c3d250c515fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dentro de training, validação e test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mclasse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolder_with_vegetable_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# os.path.join cria paths com os separadores corretos pra cada sistema operacional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# barra normal, barra invertida, isso muda do Windows pro Linux/Mac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'folder_with_vegetable_names' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing**"
      ],
      "metadata": {
        "id": "am_2scxDMOHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting image size\n",
        "image_size = 100\n",
        "\n",
        "##Transforming images: for more robust models you have to do a great job with data augmentation\n",
        "# In this case I didn't do anything other than resizing the image, but it's always good to do rotations, mirroring, random crop for good measure\n",
        "\n",
        "image_transformation = {\n",
        "    'training': transforms.Compose([\n",
        "        transforms.Resize(size=[image_size, image_size]),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(size=[image_size, image_size]),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=[image_size, image_size]),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "tFmNLNKBMQVv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the images\n",
        "# Determine the folders\n",
        "\n",
        "training_folder = dir_training\n",
        "validation_folder = dir_validation\n",
        "test_folder = dir_test\n",
        "\n",
        "training_folder, validation_folder, test_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5fLtzSFMitY",
        "outputId": "1a0f2e49-04f4-4650-c952-dc809f29d9a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./data/processed/training',\n",
              " './data/processed/validation',\n",
              " './data/processed/test')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation for the trainment**"
      ],
      "metadata": {
        "id": "Tn6v2KC3MlPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "number_of_classes = len(os.listdir(training_folder))\n",
        "number_of_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeyeaeAnMoQN",
        "outputId": "3d37d0a4-2873-4f99-8578-393e5ca53ad5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the images using torchvision datasets\n",
        "data = {\n",
        "    'training': datasets.ImageFolder(root= training_folder , transform=image_transformation['training']),\n",
        "    'validation': datasets.ImageFolder(root= validation_folder , transform=image_transformation['validation'])\n",
        "}\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "W2krVHDlM0IQ",
        "outputId": "c2ba1dfa-cbb2-4569-b9f9-3048aa406cfc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any class folder in ./data/processed/training.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a9454ee4cee0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load the images using torchvision datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m data = {\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtraining_folder\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalidation_folder\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in ./data/processed/training."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the indexes with the class names (each index will have a related fruit name)\n",
        "index_for_class = {index: classe for classe, index in data['training'].class_to_idx.items()}\n",
        "\n",
        "index_for_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "jObkZj-pM94n",
        "outputId": "aeeca2c9-f9d6-466c-c7e4-f4760a7af830"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-750597c25667>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Map the indexes with the class names (each index will have a related fruit name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex_for_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclasse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclasse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex_for_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_images_training = len(data['training'])\n",
        "num_images_validation = len(data['validation'])\n",
        "\n",
        "num_images_training, num_images_validation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "k__HtMMQNAzG",
        "outputId": "e00f9a34-f773-48a2-e894-c1cb327544e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-bc1348fe5936>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_images_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_images_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_images_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for training and validation\n",
        "# DataLoader organizes training and validation data for neural network training\n",
        "data_loader_training = DataLoader(data['training'], batch_size=batch_size, shuffle=True)\n",
        "data_loader_validation = DataLoader(data['validation'], batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "WGri6thPNUgS",
        "outputId": "d49c6af7-b917-4ef9-bb9a-edc4297b34bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-297bfe67e397>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create DataLoaders for training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# DataLoader organizes training and validation data for neural network training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_loader_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_loader_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning**"
      ],
      "metadata": {
        "id": "KFRR5sFZNXGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "alexnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfqyEqu1NYeN",
        "outputId": "9af30524-bcf5-4bc7-f3de-f03821f90e47"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:02<00:00, 88.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freeze the parameters of the pre-trained network\n",
        "#required_grad = false turns down the trainment adn the atualization\n",
        "\n",
        "for param in alexnet.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "WXRbvpppNceM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the last layer to change the number of classes\n",
        "# Remembering that we take advantage of the entire network, we only plug in one final layer  that you will effectively learn from our classes\n",
        "\n",
        "# Changing Linear(in_features=4096, out_features=1000, bias=True) for Linear(in_features=4096, out_features=4, bias=True), as we have 4 produce\n",
        "alexnet.classifier[6] = nn.Linear(4096, number_of_classes)\n",
        "\n",
        "# Including softmax, which makes the probabilities of being carrot, potato,lemon or tomato be 1 (effectively converts to probabilities to facilitate our\n",
        "## analysis)\n",
        "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1))\n",
        "\n",
        "alexnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kIK_zDaNecW",
        "outputId": "cfbad3a6-b72e-4eed-d9f7-316ec473a97f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=0, bias=True)\n",
              "    (7): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_last_layers = alexnet.classifier[6].parameters()\n",
        "\n",
        "[parameter for parameter in parameters_last_layers][0].requires_grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoDkE2uFNsiA",
        "outputId": "01d7c669-e71d-49ef-a367-03893c83540c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use the cross entropy error function, quite common for sorting issues\n",
        "function_error = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "yIR2zRfSNwAf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer** is the one who effectively changes the network weights according to some algorithm\n",
        "\n",
        "In this case we will use the Adam Optimizer, but it is possible to use Stochastic Gradient Descent, for example"
      ],
      "metadata": {
        "id": "lkfCC1JWNzZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(alexnet.parameters())\n",
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVCrghzCN1IW",
        "outputId": "927b192b-c05a-4741-ec53-6fc009cc21a8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Validate**"
      ],
      "metadata": {
        "id": "7AkZFHj6Oqtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training performs several paths forward (forward: prediction), error calculations (distance between prediction and real value) and backward (backpropagation: learning from the error). At each epoch, all training images are used to optimize the network parameters. Within each epoch, we use batch training, instead of performing image-by-image training.\n",
        "\n",
        "When validating, remember that we do not want to maintain the gradient calculation, as we will not perform backpropagation."
      ],
      "metadata": {
        "id": "qpaBWEU8OsAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, loss_metric, optimizer, epochs=10):\n",
        "    '''\n",
        "    Function for training and validation\n",
        "    Parameters\n",
        "        :param model: model to train and validate\n",
        "        :param loss_metric: loss criterion for minimization\n",
        "        :param optimizer: optimizer to update network parameters\n",
        "        :param epochs: number of epochs (default=10)\n",
        "\n",
        "    Returns\n",
        "        best_model: model trained with the best validation accuracy\n",
        "        history: (dictionary): history with training error, validation error, and accuracy\n",
        "    '''\n",
        "\n",
        "    # Initializing history\n",
        "    # Best validation accuracy\n",
        "    # Best model based on validation\n",
        "    history = []\n",
        "    best_accuracy = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    # If you are rich and have an RTX 3090 ti (what a dream!!)\n",
        "    # But if not, no problem, your CPU will probably handle it!\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    # Each epoch goes through all the training images and calculates training and validation errors\n",
        "    # for neural network learning\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"\\n\\nEpoch: {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "        # Training error and accuracy in this epoch\n",
        "        train_error = 0.0\n",
        "        train_corrects = 0.0\n",
        "\n",
        "        # Validation error and accuracy in this epoch\n",
        "        val_error = 0.0\n",
        "        val_corrects = 0.0\n",
        "\n",
        "        # Iterate over each batch of images. The inputs are the batch tensors\n",
        "        # and the labels are the classifications of each image in the batch:\n",
        "        # potato, carrot, lemon, and tomato (0, 1, 2, and 3)\n",
        "        for i, (batch_images, y_true) in enumerate(data_loader_training):\n",
        "            print(f\"\\nBatch: {i+1}\\n\")\n",
        "\n",
        "            # Send to GPU or CPU, depending on your hardware and installed pytorch\n",
        "            batch_images = batch_images.to(device)\n",
        "            y_true = y_true.to(device)\n",
        "\n",
        "            # Clear the gradients: zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass - calculate outputs from inputs using the model\n",
        "            # Since the batch has 8 images, we will have 8 predictions\n",
        "            predictions = model(batch_images)\n",
        "\n",
        "\n",
        "            '''\n",
        "            # Let's understand better what's going on!\n",
        "            print(batch_images.size()) # we have a tensor with 8 images\n",
        "            print(predictions) # we have a tensor with 8 predictions, each with 4 log-probability values\n",
        "            print(torch.exp(predictions)) # converting to exponential to get real probabilities\n",
        "\n",
        "            print(torch.max(predictions.data, 1))\n",
        "\n",
        "            # Looking at y_true\n",
        "            print(y_true)\n",
        "\n",
        "\n",
        "            # Let's take a look at the images in the batch!\n",
        "            for index in range(batch_size):\n",
        "                tensor_to_image = transforms.ToPILImage()\n",
        "                image = tensor_to_image(batch_images[index])\n",
        "                display(image)\n",
        "\n",
        "            break\n",
        "            '''\n",
        "\n",
        "            # Calculate the error of the predicted outputs in the forward pass\n",
        "            # comparing with the actual classifications (predicted vs real)\n",
        "            # and return the mean of the errors (there are 8 errors, remember?)\n",
        "            loss = loss_metric(predictions, y_true)\n",
        "\n",
        "            # Backpropagation is the combination of loss.backward() + optimizer.step()\n",
        "            # loss.backward() calculates the gradients, i.e., which direction\n",
        "            # the coefficients should go to reduce the error\n",
        "            # optimizer.step() updates the coefficients according to the gradients\n",
        "            # calculated in the previous step\n",
        "\n",
        "            # Calculate gradients from the prediction error\n",
        "            # The optimizer will use these gradients to know\n",
        "            # which direction to update the network coefficients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the network parameters according to the gradients calculated in backward\n",
        "            optimizer.step()\n",
        "\n",
        "            # From here on, we calculate the accuracy\n",
        "            # to evaluate the model's evolution during training\n",
        "            # in the epochs\n",
        "\n",
        "            # Calculate the total error for this batch and add the training error\n",
        "            # The calculated error is already the mean of the 8 errors, so we need to multiply\n",
        "            # by the number of images in the batch\n",
        "            train_error += loss.item() * batch_images.size(0)\n",
        "\n",
        "            # Accuracy calculation\n",
        "            # To calculate accuracy, we need to find the predicted classes for each image in the batch\n",
        "\n",
        "            # Each tensor is an image in the batch with three positions: probabilities of each class\n",
        "            # torch.max will return the value of the highest probability,\n",
        "            # as well as the position in the tensor (tuple output)\n",
        "            # The position, therefore, will indicate the class with the highest probability (0, 1, 2, and 3)\n",
        "            # First, find the maximum value of each tensor for each image, which will give\n",
        "            # the final predicted class\n",
        "            _, predicted_classes = torch.max(predictions.data, 1)\n",
        "            correct_preds = (predicted_classes == y_true).type(torch.FloatTensor)\n",
        "\n",
        "            # Convert the correct predictions to float and calculate the mean accuracy\n",
        "            # of the batch\n",
        "            accuracy = torch.mean(correct_preds)\n",
        "\n",
        "            # Calculate the total training accuracy of the entire batch and add it to train_acc\n",
        "            train_corrects += torch.sum(correct_preds)\n",
        "\n",
        "            #print(\"Training - Batch number {:03d}, Error: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), accuracy.item()))\n",
        "\n",
        "\n",
        "        # Validation - no need to track gradients, as the model will not be trained with validation\n",
        "        # \"turn off\" autograd\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Switch from training to validation\n",
        "            model.eval()\n",
        "\n",
        "            # Validation iteration\n",
        "            for j, (batch_images, y_true) in enumerate(data_loader_validation):\n",
        "                batch_images = batch_images.to(device)\n",
        "                y_true = y_true.to(device)\n",
        "\n",
        "                # Forward pass for validation\n",
        "                # Model prediction in this epoch\n",
        "                predictions = model(batch_images)\n",
        "\n",
        "                # Calculate validation error\n",
        "                # Predicted vs the actual fruits\n",
        "                loss = loss_metric(predictions, y_true)\n",
        "\n",
        "                # Calculate validation error and add to val_loss\n",
        "                val_error += loss.item() * batch_images.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                _, predicted_classes = torch.max(predictions.data, 1)\n",
        "                correct_preds = (predicted_classes == y_true).type(torch.FloatTensor)\n",
        "\n",
        "                # Convert the correct predictions to float and calculate the mean accuracy\n",
        "                accuracy = torch.mean(correct_preds)\n",
        "\n",
        "                # Calculate the total validation accuracy of the entire batch and add it to train_acc\n",
        "                val_corrects += torch.sum(correct_preds)\n",
        "\n",
        "                #print(\"Validation - Batch number: {:03d}, Error: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), accuracy.item()))\n",
        "\n",
        "        #break\n",
        "\n",
        "        # Calculate the mean error and accuracy in training\n",
        "        mean_train_error = train_error / num_images_training\n",
        "        mean_train_accuracy = train_corrects / num_images_training\n",
        "\n",
        "        # Calculate the mean error and accuracy in validation\n",
        "        mean_val_error = val_error / num_images_validation\n",
        "        mean_val_accuracy = val_corrects / num_images_validation\n",
        "\n",
        "        # Include in history the mean errors and accuracies\n",
        "        history.append([mean_train_error, mean_val_error, mean_train_accuracy, mean_val_accuracy])\n",
        "\n",
        "        epoch_end = time.time()\n",
        "\n",
        "        print(\"Epoch: {:03d}, Training: Error: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation: Error: {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, mean_train_error, mean_train_accuracy*100, mean_val_error, mean_val_accuracy*100, epoch_end-epoch_start))\n",
        "\n",
        "        # Check if the validation accuracy of this model in this epoch is the best\n",
        "        # If it's the best, save it as the best model and best accuracy\n",
        "        if mean_val_accuracy > best_accuracy:\n",
        "            best_accuracy = mean_val_accuracy\n",
        "            #torch.save(model, './models/best_model.pt')\n",
        "            best_model = model\n",
        "\n",
        "    return best_model, history"
      ],
      "metadata": {
        "id": "l8rtJ7OJO0EY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model!\n",
        "trained_model, history = train_and_validate(alexnet, function_error , optimizer, num_epochs)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "4amJVZ--O5Go",
        "outputId": "752e22c4-1005-4f7c-d294-48ec37372bf6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "\n",
            "\n",
            "Epoch: 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_loader_training' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-775b31dabc12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_error\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-0428ae2f616a>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, loss_metric, optimizer, epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# and the labels are the classifications of each image in the batch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# potato, carrot, lemon, and tomato (0, 1, 2, and 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nBatch: {i+1}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_loader_training' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**History of trainment e validation**"
      ],
      "metadata": {
        "id": "e2bFcbf_PFAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = np.array(history)\n",
        "\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Error training', 'Error validation'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error')\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ThzHhcR2PLZI",
        "outputId": "4f0f43a0-3480-401e-ebb5-7d1daa0bbd48"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-671bb534d4be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Error training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Error validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = np.array(history)\n",
        "\n",
        "plt.plot(history[:,2:])\n",
        "plt.legend(['Accuracy training', 'Accuracy validation'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error')\n",
        "plt.ylim(0,1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n7f6eqP9PRmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_lets_veggie(image_test):\n",
        "    '''\n",
        "    Função para realizar a predição do status do AR\n",
        "    Parâmetros\n",
        "        :param image_test: image já transformada com o PIL\n",
        "    '''\n",
        "    transformation = image_transformation['test']\n",
        "\n",
        "    tensor_test_image = transformation(image_test)\n",
        "\n",
        "    # Testa se voce é rico, ops, se tem uma placa NVIDIA :) (ou se instalou a versão certa do PyTorch)\n",
        "    if torch.cuda.is_available():\n",
        "        tensor_test_image = tensor_test_image.view(1, 3, image_size, image_size).cuda()\n",
        "    else:\n",
        "        tensor_test_image = tensor_test_image.view(1, 3, image_size, image_size)\n",
        "\n",
        "    dict_predictions = dict()\n",
        "\n",
        "    # Não precisa calcular os gradientes com o autograd\n",
        "    with torch.no_grad():\n",
        "        trained_model.eval()\n",
        "        # Modelo retorna as probabilidades em log (log softmax)\n",
        "        predictions_log = trained_model(tensor_test_image)\n",
        "\n",
        "        # torch.exp para voltar a probabilidade de log para a probabilidade linear\n",
        "        predictions = torch.exp(predictions_log)\n",
        "\n",
        "        dict_predictions = {index_for_class[classe]: float(predictions[0][classe]) for classe in range(number_of_classes)}\n",
        "\n",
        "        #print(dict_predictions)\n",
        "\n",
        "    return dict_predictions"
      ],
      "metadata": {
        "id": "sxwJuyajPUzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction on test images**"
      ],
      "metadata": {
        "id": "nul0B5EXPYCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not use the test images anywhere! So we learned that it should be and we will do it forever and ever as good and good data scientists.\n",
        "\n",
        "Let's make some predictions just to \"play around\" and then calculate the accuracy."
      ],
      "metadata": {
        "id": "enSmofddPZ9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_with_vegetable_names"
      ],
      "metadata": {
        "id": "LyzwZsngPdGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure, plots = plt.subplots(nrows=number_of_classes, ncols=1, figsize=(5, 10))\n",
        "\n",
        "for index, classe in enumerate(folder_with_vegetable_names):\n",
        "    folder_test_class = os.path.join(folder_test, classe)\n",
        "    image = os.path.join(folder_test_class, os.listdir(folder_test_class)[3])\n",
        "    image_teste = PIL.Image.open(image)\n",
        "\n",
        "    plots[index].imshow(image_teste)\n",
        "\n",
        "    prediction = prediction_lets_veggie(image_teste)\n",
        "\n",
        "    display(max(prediction, key=prediction.get))"
      ],
      "metadata": {
        "id": "TDI1w8KiPfMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que perpassa todas as imagens na pasta de teste\n",
        "# Realiza a predição utilizando a função prediction_lets_veggie\n",
        "# e calcula a acurácia (total corrects/total imagens)\n",
        "def accuracy_test(test_folder):\n",
        "    corrects = 0\n",
        "    images_total = 0\n",
        "\n",
        "    for classe in os.listdir(test_folder):\n",
        "        complete_folder = os.path.join(test_folder, classe)\n",
        "        images_total += len(os.listdir(complete_folder))\n",
        "\n",
        "        for image in os.listdir(complete_folder):\n",
        "            image = os.path.join(complete_folder, image)\n",
        "            image_teste = PIL.Image.open(image)\n",
        "            predictions = prediction_lets_veggie(image_teste)\n",
        "            prediction = max(predictions, key=predictions.get)\n",
        "\n",
        "            if prediction == classe:\n",
        "                corrects += 1\n",
        "\n",
        "    return 100 * corrects/images_total"
      ],
      "metadata": {
        "id": "IY_iPj1wPlmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test(test_folder)"
      ],
      "metadata": {
        "id": "7X0hbpUCPno9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Deploy**"
      ],
      "metadata": {
        "id": "XEZNLOioPqlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gradio_interface = gr.Interface(\n",
        "    fn=prediction_lets_veggie,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=gr.Label()\n",
        ")\n",
        "\n",
        "gradio_interface.launch(share=True)"
      ],
      "metadata": {
        "id": "iBTWPnAyPtj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}